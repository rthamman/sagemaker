{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Summarization of Medical Articles Using Sum Basic\n",
    "### Author: Rama Thamman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: In this notebook we'll use Sum Basic for automatic summarization of medical articles. NIH's (National Institues for Health) PubMed repository consists of links to hundreds of thousands of medical articles. We will use articles relevant to various types of cancer. We will use the abstract of each article as the \"ground truth\". We will apply the Sum Basic algorithm to only the body of the PubMed article without the abstract to generate an extractive summary. We will use a Java based implementation of ROUGE software to evaluate the precision, recall and F1 score of extractive summary with respect to the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1: Import required modules__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from pyparsing import ZeroOrMore, Regex\n",
    "import csv\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2: Generate a list of documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1994795/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=1994795')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314300/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=314300')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4383356/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4383356')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4596899/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4596899')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4303126/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4303126')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4637461/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4637461')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4690355/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4690355')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3505152/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=3505152')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3976810/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=3976810')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4061037/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4061037')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3: Extract abstracts and document body__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "abstracts = []\n",
    "texts = []\n",
    "print 'Preprocessing documents. This may take a few minutes ...'\n",
    "for i, url in enumerate(urls):\n",
    "    print 'Preprocessing document %d ...' % (i+1)\n",
    "    # Download the document\n",
    "    my_url = urllib2.urlopen(url)\n",
    "    raw_doc = BeautifulSoup(my_url.read(), 'xml')\n",
    "    documents.append(raw_doc)\n",
    "\n",
    "    # Extract the cleaned abstract\n",
    "    raw_abstract = raw_doc.abstract\n",
    "    my_abstract = re.sub(r'<\\/?\\w+>', r' ', str(raw_abstract)) # remove xml tags\n",
    "    abstracts.append(my_abstract)\n",
    "\n",
    "    # Extract the cleaned text\n",
    "    text = raw_doc.find_all('sec') \n",
    "    text = re.sub(r'\\\\n', r' ', str(text)) # remove newline characters\n",
    "    text = re.sub(r'<[^>]+>', r' ', str(text)) # remove xml tags\n",
    "    text = re.sub(r'\\[[^\\[^\\]]+\\]', r' ', str(text)) # remove references\n",
    "    text = re.sub(r'\\[', r' ', str(text)) # remove any remaining [\n",
    "    text = re.sub(r'\\]', r' ', str(text)) # remove any remaining ]\n",
    "    text = re.sub(r'[\\s]{2,}', r' ', str(text)) # remove more than a single blank space\n",
    "    text = re.sub(r'\\.\\s+,\\s+\\S', r' ', str(text)) # remove , after a period\n",
    "\n",
    "    text = text.decode('utf-8')\n",
    "    texts.append(text)\n",
    "\n",
    "print 'All documents preprocessed successfully.'\n",
    "print 'We have %d documents with %d abstracts and %d texts.' % (len(documents), len(abstracts), len(texts))\n",
    "assert len(documents) == len(abstracts)\n",
    "assert len(documents) == len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4: Split the documents into sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punkttokenizer = PunktSentenceTokenizer()\n",
    "text_sentences = []\n",
    "\n",
    "for text in texts:\n",
    "    sentences =  []\n",
    "    seen = set()\n",
    "    for sentence in punkttokenizer.tokenize(text):\n",
    "        if sentence in seen:\n",
    "            pass\n",
    "        else:\n",
    "            seen.add(sentence)\n",
    "            sentences.append(sentence)\n",
    "    text_sentences.append(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Step 5: Configure stop words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop words\n",
    "words_to_ignore = set(stopwords.words('english'))\n",
    "words_to_ignore.add('[pubmed]')\n",
    "words_to_ignore.add('[pmc free article]')\n",
    "words_to_ignore.add('[cross ref]')\n",
    "words_to_ignore.add('et')\n",
    "words_to_ignore.add('al.')\n",
    "words_to_ignore.add('figure')\n",
    "words_to_ignore.add('fig')\n",
    "words_to_ignore.add('fig.')\n",
    "\n",
    "words_to_ignore_contains = ['doi','dg']\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Step 6: Extact content words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_content_words = []\n",
    "stemmer = PorterStemmer()\n",
    "remove_stop_words=False\n",
    "stem_words=False\n",
    "\n",
    "def get_content_words(sentences):\n",
    "    processed_words = []\n",
    "    parser = ZeroOrMore(Regex(r'\\[[^]]*\\]') | Regex(r'\"[^\"]*\"') | Regex(r'[^ ]+'))| Regex(r'\"(^\")*\"')\n",
    "    for ii in range(len(sentences)):\n",
    "        words = parser.parseString(sentences[ii])\n",
    "        for jj in range(len(words)):\n",
    "            word = words[jj]\n",
    "            word = word.lower()\n",
    "            #remove stop words\n",
    "            if remove_stop_words == True and word in words_to_ignore:\n",
    "                continue\n",
    "            # stem words\n",
    "            if stem_words == True:\n",
    "                word = stemmer.stem(word)\n",
    "         \n",
    "            word = canonicalize_word(word)\n",
    "            if word != '':\n",
    "                processed_words.append(word)\n",
    "    return processed_words\n",
    "\n",
    "def canonicalize_word(word):\n",
    "    #check for words with just special characters\n",
    "    if re.match(r'^[_\\W]+$', word):\n",
    "        word = \"\"\n",
    "    #replace numbers with DG\n",
    "    word = re.sub(\"\\d+\", \"dg\", word)\n",
    "    if word.startswith(\"dg\") and not any([c.isalpha() for c in word]):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    #check for ignore words \n",
    "    for w in words_to_ignore_contains:\n",
    "        if word.find(w) != -1:\n",
    "            word = \"\"\n",
    "            break;\n",
    "    return word\n",
    "\n",
    "# get content words for all documents\n",
    "print 'Extracting content words for each document. This may take a few minutes ...'\n",
    "for ii in range(len(text_sentences)):\n",
    "    sentences = text_sentences[ii]\n",
    "    c_words = get_content_words(sentences)\n",
    "    doc_content_words.append(c_words)\n",
    "    print \"Word count for #\",ii+1,\" - \",len(c_words)\n",
    "    #print c_words\n",
    "\n",
    "assert len(text_sentences), len(doc_content_words)\n",
    "print 'All documents processed successfully.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Step 8: Compute word probability__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute word probability\n",
    "docs_word_probability = []\n",
    "docs_word_frequency=[]\n",
    "print 'Calculating word probability. This may take a few minutes ...'\n",
    "for ii in range(len(text_sentences)):    \n",
    "    content_words_freq = {}\n",
    "    content_words_tf = {}\n",
    "    words = doc_content_words[ii]\n",
    "    for w in words:\n",
    "        content_words_freq[w] = content_words_freq.get(w, 0) + 1  \n",
    "    content_words_count = len(doc_content_words[ii])\n",
    "    for (k, v) in content_words_freq.items():\n",
    "        content_words_tf[k] = v/float(content_words_count)\n",
    "    docs_word_probability.append(content_words_tf)\n",
    "    docs_word_frequency.append(content_words_freq)\n",
    "    \n",
    "    #top5 = sorted(content_words_tf, key=content_words_tf.get, reverse=True)[:5]\n",
    "    #for top_word in top5:\n",
    "    #    print top_word,\" score:\"+str(docs_word_probability[ii][top_word]),\" count:\", docs_word_frequency[ii][top_word]\n",
    "    #print \"#\" * 30\n",
    "assert len(text_sentences), len(docs_word_probability)\n",
    "assert len(text_sentences), len(docs_word_frequency)\n",
    "print 'Done.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 8: Utility methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_best_stats(sentence, word_probability, words):\n",
    "    score = float('-inf')\n",
    "    best_word = \"\"\n",
    "    for w in words:\n",
    "        if word_probability[w] > score:\n",
    "            score = word_probability[w]\n",
    "            best_word = w\n",
    "    print \"best sentence: \",best_sentence\n",
    "    print \"best sentence words: \", words      \n",
    "    print best_word ,\" \",score\n",
    "    print \"#\" * 30\n",
    "    \n",
    "# Utility methods\n",
    "def read_file(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    return doc\n",
    "\n",
    "def write_to_file(score, file_name='output.txt'):\n",
    "    with open(file_name, 'wb') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in score.items():\n",
    "            values = value.split(',')        \n",
    "            writer.writerow([key, values[0], values[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 8: Apply Sum Basic__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute sentence ratings\n",
    "num_sentence_summaries=10\n",
    "min_content_words=5\n",
    "doc_summary = []\n",
    "verbose= False\n",
    "write_log_to_file = False\n",
    "\n",
    "# Iterate for each doucment\n",
    "print 'Appying Sum Basic model. This may take a few minutes ...'\n",
    "for ii in range(len(text_sentences)):\n",
    "    print 'Processing doucment', ii+1\n",
    "    sentences = copy.deepcopy(text_sentences[ii])\n",
    "    content_words = doc_content_words[ii]\n",
    "    word_probability = docs_word_probability[ii]\n",
    "    ratings = {}\n",
    "    \n",
    "    # Iterate for number summaries required\n",
    "    for jj in range(num_sentence_summaries):\n",
    "        max_value = float(\"-inf\")\n",
    "        best_sentence_index = 0\n",
    "        log = {}\n",
    "        # Iterate for each sentence\n",
    "        for kk, sentence in enumerate(sentences):\n",
    "            words = get_content_words([sentence])\n",
    "            word_freq_avg = 0\n",
    "            if len(words) >= min_content_words:\n",
    "                word_freq_sum = sum([word_probability[w] for w in words])\n",
    "                word_freq_avg = word_freq_sum / len(words)\n",
    "            if word_freq_avg > max_value:\n",
    "                max_value = word_freq_avg\n",
    "                best_sentence_index = kk\n",
    "            if write_log_to_file:\n",
    "                content_words_str = \" \".join(list(words))\n",
    "                log[sentence.encode('utf-8')]= str(word_freq_avg) + \" , \" + content_words_str\n",
    "\n",
    "        if write_log_to_file:\n",
    "            write_to_file(log, \"log\"+str(ii)+\"-\"+str(jj)+\".csv\")\n",
    "\n",
    "        best_sentence = sentences.pop(best_sentence_index)\n",
    "        ratings[best_sentence] = -len(ratings)\n",
    "        \n",
    "        words = get_content_words([best_sentence])\n",
    "      \n",
    "        if verbose:\n",
    "            print_best_stats(best_sentence, word_probability, words)\n",
    "            \n",
    "        # update probability\n",
    "        for w in words:\n",
    "            word_probability[w] *= word_probability[w]\n",
    "    doc_summary.append(ratings)\n",
    "print 'All documents processed successfully.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 9: Write summaries and ground truth to file __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Saving summaries from Sum Basic model. This may take a few minutes ...'\n",
    "for ii in range(len(text_sentences)):\n",
    "    #print 'Writing extractive summary for document %d ...' % (ii+1)\n",
    "    sentence_ranking = doc_summary[ii]\n",
    "    sorted_sentence_ranking = sorted(sentence_ranking.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_sentence_ranking_list = list(sorted_sentence_ranking) \n",
    "    out_file = '.\\\\rouge\\\\system\\\\article%d_system1.txt' % (ii+1)\n",
    "    with open(out_file, 'w') as f:\n",
    "        for jj in range (len(sorted_sentence_ranking_list)):\n",
    "            f.write(sorted_sentence_ranking_list[jj][0])\n",
    "    \n",
    "for ii, abstract in enumerate(abstracts):    \n",
    "    #print 'Writing ground truth for document %d ...' % (ii+1)\n",
    "    out_file = 'rouge\\\\reference\\\\article%d_reference1.txt' % (ii+1)\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write(abstract.strip())\n",
    "print 'All documents processed successfully.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 10: Calculate F Score using Rouge __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd rouge\n",
    "!java -jar rouge2.0_0.2.jar\n",
    "!type results.csv\n",
    "%cd .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
